============================================================
iPhone(撮影/送信)アプリ & iPad(受信/再生)アプリ：実装タスクリスト（MVP）
============================================================

前提（MVPの守備範囲）
- 操作ゼロを最優先（録画/再生ボタンなし）
- 準ライブ：3〜5秒遅延で常時再生
- 映像：540p (960x540), 24fps, ~1Mbps
- 音声：送らない
- 通信：Multipeer Connectivity（端末間直接通信）
- まずは iPhone(撮影) × iPad(受信) のみ。受信iPhone対応は次フェーズ。

------------------------------------------------------------
A. iPhoneアプリ（撮影・エンコード・送信）タスク
------------------------------------------------------------

A0. プロジェクト基盤
[x] A0-1: Xcodeプロジェクト作成（iPhone専用）
[x] A0-2: 権限設定（Info.plist: NSCameraUsageDescription など）
[x] A0-3: ログ基盤（OSLog）/ デバッグ表示（接続状態だけ）

A1. Multipeer接続（送信側）
[x] A1-1: MCPeerID生成（端末名ベース）
[x] A1-2: MCSession作成（delegate実装）
[x] A1-3: MCNearbyServiceAdvertiser 起動（常時）
[x] A1-4: 接続状態ステート管理（Searching/Connecting/Connected/Reconnecting/Error）
[x] A1-5: 切断検知 → 再Advertise/再接続フロー（最低限）

A2. カメラキャプチャ（映像取得）
[x] A2-1: AVCaptureSession 構築（背面カメラ）
[x] A2-2: 解像度設定（可能なら 960x540 相当）
[x] A2-3: FPS設定（可能なら 24fps 固定）
[x] A2-4: AVCaptureVideoDataOutputでCMSampleBuffer取得
[x] A2-5: フレーム落ち/遅延の基本監視（フレーム間隔ログなど）

A3. 低遅延H.264エンコード（VideoToolbox）
[x] A3-1: VTCompressionSession 作成
[x] A3-2: リアルタイム/低遅延向けプロパティ設定（方針）
        - リアルタイム
        - Bフレーム抑制（遅延増回避）
        - キーフレーム間隔設定（復帰性）
        - ビットレート ~1Mbps
[x] A3-3: CMSampleBuffer → H.264(NALU)出力取得
[x] A3-4: 送信用に「フレーム単位」のデータ構造へ整形
[x] A3-5: 破損/変換失敗時はそのフレームを捨てる（MVP方針）

A4. パケット化・送信（MCSession）
[x] A4-1: 送信データのヘッダ設計（最低限）
        - timestamp(またはframeIndex)
        - chunkIndex / chunkCount（分割用）
        - payloadSize
[x] A4-2: 最大サイズを超える場合の分割実装（chunking）
[x] A4-3: 送信API実装（reliable/unreliableは実機で評価）
[x] A4-4: 送信レート制御（詰まるならフレーム間引き許容）
[x] A4-5: 通信量/遅延の簡易メトリクス（送信FPS/キュー長ログ）

A5. 実機テスト（送信側）
[ ] A5-1: iPad受信アプリと接続できる（初回〜再接続）
[ ] A5-2: 10分連続送信で破綻しない（熱・落ちない）
[ ] A5-3: 距離/遮蔽で切断→復帰する（最低限）

------------------------------------------------------------
B. iPadアプリ（受信・遅延バッファ・デコード・表示）タスク
------------------------------------------------------------

B0. プロジェクト基盤
[x] B0-1: Xcodeプロジェクト作成（iPad専用）
[x] B0-2: UI最小：全画面表示 + 状態ラベル（Waiting/Connectedなど）
[x] B0-3: ログ基盤（OSLog）/ デバッグ表示（受信FPS/バッファ量）
[x] B0-4: UIは画面サイズ非依存で設計する
        - SwiftUI + GeometryReader 前提
        - 固定サイズのボタン・固定位置オーバーレイは作らない
        - 将来の受信iPhone対応でUIを書き直さないため



B1. Multipeer接続（受信側）
[x] B1-1: MCPeerID生成
[x] B1-2: MCSession作成（delegate実装）
[x] B1-3: MCNearbyServiceBrowser 起動（常時）
[x] B1-4: 接続状態ステート管理（Searching/Connecting/Connected/Reconnecting/Error）
[x] B1-5: 切断検知 → 再Browse/再接続フロー（最低限）

B2. 受信データの復元（reassemble）
[x] B2-1: 受信ハンドラでデータ受領
[x] B2-2: ヘッダ解析（frame/timestamp, chunkIndex/chunkCount）
[x] B2-3: chunk結合 → 1フレーム（H.264データ）復元
[x] B2-4: 欠損chunkがあるフレームは破棄（MVP方針）
[x] B2-5: 復元キューを過負荷時に間引く（メモリ保護）

B3. 遅延バッファ（3〜5秒）
[x] B3-1: DelayBuffer（リング or キュー）実装
[x] B3-2: 「一定量溜まるまで再生開始しない」ゲート
[x] B3-3: バッファ枯渇時の挙動：一旦待機に戻す（フリーズ回避）
[x] B3-4: 遅延秒数の調整ポイントを1箇所に集約（将来の調整容易化）

B4. H.264デコード（VideoToolbox）
[x] B4-1: SPS/PPSの扱い（キーフレームから復帰できるように）
[x] B4-2: VTDecompressionSession 作成
[x] B4-3: H.264データ → CVPixelBuffer（またはCMSampleBuffer）へ
[x] B4-4: 破損フレームは捨てる（復元しない）
[x] B4-5: デコード遅延が出る場合は入力間引き（MVP方針）

B5. 表示（低遅延）
[x] B5-1: AVSampleBufferDisplayLayer を使った描画（推奨）
[x] B5-2: SwiftUIラッパ（UIViewRepresentable）で全画面表示
[x] B5-3: 回転/アスペクト（fill/fit）ポリシー決定
[x] B5-4: 描画が詰まったら古いフレームを捨てる（最新優先）

B6. フェイル設計（MVP必須）
[x] B6-1: 一定時間受信なし → 待機画面 + 再接続トライ
[x] B6-2: バッファ破綻（溢れ/枯れ）時の安全動作
[x] B6-3: 例外時に落とさない（assertよりログ・復帰）
[x] B6-4: 長時間運用（10分〜）で破綻しない

------------------------------------------------------------
C. 統合テスト（MVP Done の定義）
------------------------------------------------------------

C1. 体験（UX）
[ ] C1-1: 両アプリ起動後、操作なしで映像が流れる
[ ] C1-2: 遅延が3〜5秒程度で安定（厳密でなくて良い）
[ ] C1-3: 音声なしで安定動作する

C2. 安定性
[ ] C2-1: 5〜10分連続で落ちない・固まらない
[ ] C2-2: 一時的な切断から復帰できる（完全自動でなくても「使える」）
[ ] C2-3: 画質/滑らかさより「止まらない」を優先できている

------------------------------------------------------------
D. 次フェーズ（MVP後）
------------------------------------------------------------
[ ] D1: 受信アプリのiPhone対応（UIの画面サイズ対応＋熱対策）
[ ] D2: 手動保存（または打球音トリガー保存）
[ ] D3: スロー再生、一時停止、ライン描画
[ ] D4: 保存用 720p/60fps パイプライン（必要時のみ）
